{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purpose: import from other notebook...\n",
    "#Ref: http://jupyter-notebook.readthedocs.io/en/latest/examples/Notebook/Importing%20Notebooks.html\n",
    "import io, os, sys, types\n",
    "from IPython import get_ipython\n",
    "from nbformat import read\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "#--------------------------------------------------------------------------------\n",
    "def find_notebook(fullname, path=None):\n",
    "    \"\"\"find a notebook, given its fully qualified name and an optional path\n",
    "    \n",
    "    This turns \"foo.bar\" into \"foo/bar.ipynb\"\n",
    "    and tries turning \"Foo_Bar\" into \"Foo Bar\" if Foo_Bar\n",
    "    does not exist.\n",
    "    \"\"\"\n",
    "    name = fullname.rsplit('.', 1)[-1]\n",
    "    if not path:\n",
    "        path = ['']\n",
    "    for d in path:\n",
    "        nb_path = os.path.join(d, name + \".ipynb\")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "        # let import Notebook_Name find \"Notebook Name.ipynb\"\n",
    "        nb_path = nb_path.replace(\"_\", \" \")\n",
    "        if os.path.isfile(nb_path):\n",
    "            return nb_path\n",
    "#--------------------------------------------------------------------------------\n",
    "class NotebookLoader(object):\n",
    "    \"\"\"Module Loader for Jupyter Notebooks\"\"\"\n",
    "    def __init__(self, path=None):\n",
    "        self.shell = InteractiveShell.instance()\n",
    "        self.path = path\n",
    "    \n",
    "    def load_module(self, fullname):\n",
    "        \"\"\"import a notebook as a module\"\"\"\n",
    "        path = find_notebook(fullname, self.path)\n",
    "        \n",
    "        print (\"importing Jupyter notebook from %s\" % path)\n",
    "                                       \n",
    "        # load the notebook object\n",
    "        with io.open(path, 'r', encoding='utf-8') as f:\n",
    "            nb = read(f, 4)\n",
    "        \n",
    "        \n",
    "        # create the module and add it to sys.modules\n",
    "        # if name in sys.modules:\n",
    "        #    return sys.modules[name]\n",
    "        mod = types.ModuleType(fullname)\n",
    "        mod.__file__ = path\n",
    "        mod.__loader__ = self\n",
    "        mod.__dict__['get_ipython'] = get_ipython\n",
    "        sys.modules[fullname] = mod\n",
    "        \n",
    "        # extra work to ensure that magics that would affect the user_ns\n",
    "        # actually affect the notebook module's ns\n",
    "        save_user_ns = self.shell.user_ns\n",
    "        self.shell.user_ns = mod.__dict__\n",
    "        \n",
    "        try:\n",
    "          for cell in nb.cells:\n",
    "            if cell.cell_type == 'code':\n",
    "                # transform the input to executable Python\n",
    "                code = self.shell.input_transformer_manager.transform_cell(cell.source)\n",
    "                # run the code in themodule\n",
    "                exec(code, mod.__dict__)\n",
    "        finally:\n",
    "            self.shell.user_ns = save_user_ns\n",
    "        return mod\n",
    "#--------------------------------------------------------------------------------\n",
    "class NotebookFinder(object):\n",
    "    \"\"\"Module finder that locates Jupyter Notebooks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.loaders = {}\n",
    "    \n",
    "    def find_module(self, fullname, path=None):\n",
    "        nb_path = find_notebook(fullname, path)\n",
    "        if not nb_path:\n",
    "            return\n",
    "        \n",
    "        key = path\n",
    "        if path:\n",
    "            # lists aren't hashable\n",
    "            key = os.path.sep.join(path)\n",
    "        \n",
    "        if key not in self.loaders:\n",
    "            self.loaders[key] = NotebookLoader(path)\n",
    "        return self.loaders[key]\n",
    "#--------------------------------------------------------------------------------\n",
    "sys.meta_path.append(NotebookFinder())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from cm_utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from cm_utils import split_data\n",
    "import math, random, re, glob\n",
    "#--------------------------------------------------------------------------------\n",
    "def tokenize(message):\n",
    "    message = message.lower()                       # convert to lowercase\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)   # extract the words\n",
    "    return set(all_words)                           # remove duplicates\n",
    "#--------------------------------------------------------------------------------\n",
    "def count_words(training_set):\n",
    "    \"\"\"training set consists of pairs (message, is_spam)\"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "    return counts\n",
    "#--------------------------------------------------------------------------------\n",
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"turn the word_counts into a list of triplets \n",
    "    w, p(w | spam) and p(w | ~spam)\"\"\"\n",
    "    return [(w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k))\n",
    "             for w, (spam, non_spam) in iter(counts.items())] #counts.iteritems()] #python2x\n",
    "#--------------------------------------------------------------------------------\n",
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "\n",
    "        # for each word in the message, \n",
    "        # add the log probability of seeing it \n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "\n",
    "        # for each word that's not in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "            \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "    #----------------------------------------------------------------------------\n",
    "    def train(self, training_set):\n",
    "    \n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam \n",
    "                         for message, is_spam in training_set \n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "\n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts, \n",
    "                                             num_spams, \n",
    "                                             num_non_spams,\n",
    "                                             self.k)\n",
    "    #----------------------------------------------------------------------------                                        \n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def get_subject_data(path):\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # regex for stripping out the leading \"Subject:\" and any spaces after it\n",
    "    subject_regex = re.compile(r\"^Subject:\\s+\")\n",
    "\n",
    "    # glob.glob returns every filename that matches the wildcarded path\n",
    "    for fn in glob.glob(path):\n",
    "        is_spam = \"ham\" not in fn\n",
    "        \n",
    "        with open(fn,'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith(\"Subject:\"):\n",
    "                    subject = subject_regex.sub(\"\", line).strip()\n",
    "                    data.append((subject, is_spam))\n",
    "\n",
    "    return data\n",
    "#--------------------------------------------------------------------------------\n",
    "def p_spam_given_word(word_prob):\n",
    "    word, prob_if_spam, prob_if_not_spam = word_prob\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "#--------------------------------------------------------------------------------\n",
    "def train_and_test_model(path):\n",
    "\n",
    "    data = get_subject_data(path)\n",
    "    random.seed(0)      # just so you get the same answers as me\n",
    "    train_data, test_data = split_data(data, 0.75)    \n",
    "\n",
    "    classifier = NaiveBayesClassifier()\n",
    "    classifier.train(train_data)\n",
    "\n",
    "    classified = [(subject, is_spam, classifier.classify(subject))\n",
    "              for subject, is_spam in test_data]\n",
    "\n",
    "    counts = Counter((is_spam, spam_probability > 0.5) # (actual, predicted)\n",
    "                     for _, is_spam, spam_probability in classified)\n",
    "\n",
    "    print(counts)\n",
    "\n",
    "    classified.sort(key=lambda row: row[2])\n",
    "    #spammiest_hams = filter(lambda row: not row[1], classified)[-5:] #python2x\n",
    "    spammiest_hams = list(filter(lambda row: not row[1], classified))[-5:] #python3x, we should convert a iterable filter object to a list\n",
    "    hammiest_spams = list(filter(lambda row: row[1], classified))[:5]\n",
    "\n",
    "    print(\"spammiest_hams\", spammiest_hams)\n",
    "    print(\"hammiest_spams\", hammiest_spams)\n",
    "\n",
    "    words = sorted(classifier.word_probs, key=p_spam_given_word)\n",
    "\n",
    "    spammiest_words = words[-5:]\n",
    "    hammiest_words = words[:5]\n",
    "\n",
    "    print(\"spammiest_words\", spammiest_words)\n",
    "    print(\"hammiest_words\", hammiest_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter()\n",
      "spammiest_hams []\n",
      "hammiest_spams []\n",
      "spammiest_words []\n",
      "hammiest_words []\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_and_test_model(r\"c:\\spam\\*\\*\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
